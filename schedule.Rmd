---
title: "Schedule"
---

<table>

  <tr>
    <th>Date</th>
    <th>Readings</th>
    <th>Due</th>
  </tr>
  <tr>
    <td>1/4</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Allen Institute for AI. <i>Ask Delphi</i>. 2021, <a href="https://delphi.allenai.org/">https://delphi.allenai.org/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=Ask%20Delphi&amp;rft.identifier=https%3A%2F%2Fdelphi.allenai.org%2F&amp;rft.aucorp=Allen%20Institute%20for%20AI&amp;rft.au=undefined&amp;rft.date=2021"></span>
  <div class="csl-entry">Bonde, Sheila, and Paul Firenze. <i>A Framework for Making Ethical Decisions</i>. May 2013, <a href="https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions">https://www.brown.edu/academics/science-and-technology-studies/framework-making-ethical-decisions</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=A%20Framework%20for%20Making%20Ethical%20Decisions&amp;rft.identifier=https%3A%2F%2Fwww.brown.edu%2Facademics%2Fscience-and-technology-studies%2Fframework-making-ethical-decisions&amp;rft.aufirst=Sheila&amp;rft.aulast=Bonde&amp;rft.au=Sheila%20Bonde&amp;rft.au=Paul%20Firenze&amp;rft.date=2013-05"></span>
  <div class="csl-entry">Gupta, Abhishek, and Victoria Heath. “AI Ethics Groups Are Repeating One of Society’s Classic Mistakes.” <i>MIT Technology Review</i>, Sept. 2020, <a href="https://www.technologyreview.com/2020/09/14/1008323/ai-ethics-representation-artificial-intelligence-opinion/">https://www.technologyreview.com/2020/09/14/1008323/ai-ethics-representation-artificial-intelligence-opinion/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=AI%20ethics%20groups%20are%20repeating%20one%20of%20society%E2%80%99s%20classic%20mistakes&amp;rft.source=MIT%20Technology%20Review&amp;rft.description=Too%20many%20councils%20and%20advisory%20boards%20still%20consist%20mostly%20of%20people%20based%20in%20Europe%20or%20the%20United%20States.&amp;rft.identifier=https%3A%2F%2Fwww.technologyreview.com%2F2020%2F09%2F14%2F1008323%2Fai-ethics-representation-artificial-intelligence-opinion%2F&amp;rft.aufirst=Abhishek&amp;rft.aulast=Gupta&amp;rft.au=Abhishek%20Gupta&amp;rft.au=Victoria%20Heath&amp;rft.date=2020-09-14&amp;rft.language=en"></span>
  <div class="csl-entry">Jiang, Liwei. “Towards Machine Ethics and Norms.” <i>AI2 Blog</i>, 4 Nov. 2021, <a href="https://medium.com/ai2-blog/towards-machine-ethics-and-norms-d64f2bdde6a3">https://medium.com/ai2-blog/towards-machine-ethics-and-norms-d64f2bdde6a3</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Towards%20Machine%20Ethics%20and%20Norms&amp;rft.description=Making%20machines%20more%20inclusive%2C%20ethically-informed%2C%20and%20socially-aware&amp;rft.identifier=https%3A%2F%2Fmedium.com%2Fai2-blog%2Ftowards-machine-ethics-and-norms-d64f2bdde6a3&amp;rft.aufirst=Liwei&amp;rft.aulast=Jiang&amp;rft.au=Liwei%20Jiang&amp;rft.date=2021-11-04&amp;rft.language=en"></span>
  <div class="csl-entry">Raji, Deborah. “That’s Not Fair!” <i>XRDS: Crossroads, The ACM Magazine for Students</i>, vol. 25, no. 3, Apr. 2019, pp. 44–48. <i>Spring 2019</i>, <a href="https://doi.org/10.1145/3313127">https://doi.org/10.1145/3313127</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F3313127&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=That's%20not%20fair!&amp;rft.jtitle=XRDS%3A%20Crossroads%2C%20The%20ACM%20Magazine%20for%20Students&amp;rft.stitle=XRDS&amp;rft.volume=25&amp;rft.issue=3&amp;rft.aufirst=Deborah&amp;rft.aulast=Raji&amp;rft.au=Deborah%20Raji&amp;rft.date=2019-04-10&amp;rft.pages=44%E2%80%9348&amp;rft.spage=44&amp;rft.epage=48&amp;rft.issn=1528-4972"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>1/6</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">ACLU of Washington. “Automated Decision Making Systems Are Making Some of the Most Important Life Decisions For You, but You Might Not Even Know It.” <i>ACLU of Washington</i>, 22 Sept. 2021, <a href="https://www.aclu-wa.org/story/automated-decision-making-systems-are-making-some-most-important-life-decisions-you-you-might">https://www.aclu-wa.org/story/automated-decision-making-systems-are-making-some-most-important-life-decisions-you-you-might</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Automated%20Decision%20Making%20Systems%20Are%20Making%20Some%20of%20the%20Most%20Important%20Life%20Decisions%20For%20You%2C%20but%20You%20Might%20Not%20Even%20Know%20It&amp;rft.description=Every%20day%2C%20people%20are%20denied%20healthcare%2C%20overpoliced%2C%20kept%20in%20jail%2C%20and%20passed%20up%20for%20jobs%20because%20of%20decisions%20made%20or%20aided%20by%20computers.&amp;rft.identifier=https%3A%2F%2Fwww.aclu-wa.org%2Fstory%2Fautomated-decision-making-systems-are-making-some-most-important-life-decisions-you-you-might&amp;rft.aulast=ACLU%20of%20Washington&amp;rft.au=ACLU%20of%20Washington&amp;rft.date=2021-09-22&amp;rft.language=en"></span>
  <div class="csl-entry">Kearns, Michael, and Aaron Roth. “Introduction.” <i>The Ethical Algorithm: The Science of Socially Aware Algorithm Design</i>, Oxford University Press USA - OSO, 2019. <i>ProQuest Ebook Central</i>, <a href="http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5905172">http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5905172</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-0-19-094822-1&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Introduction&amp;rft.place=Oxford%2C%20UNITED%20STATES&amp;rft.publisher=Oxford%20University%20Press%20USA%20-%20OSO&amp;rft.aufirst=Michael&amp;rft.aulast=Kearns&amp;rft.au=Michael%20Kearns&amp;rft.au=Aaron%20Roth&amp;rft.date=2019&amp;rft.isbn=978-0-19-094822-1"></span>
  <div class="csl-entry">Lum, Kristian, and Rumman Chowdhury. “What Is an ‘Algorithm’? It Depends Whom You Ask.” <i>MIT Technology Review</i>, Feb. 2021, <a href="https://www.technologyreview.com/2021/02/26/1020007/what-is-an-algorithm/">https://www.technologyreview.com/2021/02/26/1020007/what-is-an-algorithm/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=What%20is%20an%20%E2%80%9Calgorithm%E2%80%9D%3F%20It%20depends%20whom%20you%20ask&amp;rft.source=MIT%20Technology%20Review&amp;rft.identifier=https%3A%2F%2Fwww.technologyreview.com%2F2021%2F02%2F26%2F1020007%2Fwhat-is-an-algorithm%2F&amp;rft.aufirst=Kristian&amp;rft.aulast=Lum&amp;rft.au=Kristian%20Lum&amp;rft.au=Rumman%20Chowdhury&amp;rft.date=2021-02-26"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>1/11</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Bembeneck, Emily, et al. “To Stop Algorithmic Bias, We First Have to Define It.” <i>Brookings</i>, 21 Oct. 2021, <a href="https://www.brookings.edu/research/to-stop-algorithmic-bias-we-first-have-to-define-it/">https://www.brookings.edu/research/to-stop-algorithmic-bias-we-first-have-to-define-it/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=To%20stop%20algorithmic%20bias%2C%20we%20first%20have%20to%20define%20it&amp;rft.description=Emily%20Bembeneck%2C%20Ziad%20Obermeyer%2C%20and%20Rebecca%20Nissan%20lay%20out%20how%20to%20define%20algorithmic%20bias%20in%20AI%20systems%20and%20the%20best%20possible%20interjections.&amp;rft.identifier=https%3A%2F%2Fwww.brookings.edu%2Fresearch%2Fto-stop-algorithmic-bias-we-first-have-to-define-it%2F&amp;rft.aufirst=Emily&amp;rft.aulast=Bembeneck&amp;rft.au=Emily%20Bembeneck&amp;rft.au=Rebecca%20Nissan&amp;rft.au=Ziad%20Obermeyer&amp;rft.date=2021-10-21&amp;rft.language=en-US"></span>
  <div class="csl-entry">Benjamin, Ruha. “Default Discrimination.” <i>Race after Technology: Abolitionist Tools for the New Jim Code</i>, Polity Press, 2019. <i>ProQuest Ebook Central</i>, <a href="http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5820427">http://ebookcentral.proquest.com/lib/washington/detail.action?docID=5820427</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-5095-2643-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Default%20Discrimination&amp;rft.place=Newark%2C%20UNITED%20KINGDOM&amp;rft.publisher=Polity%20Press&amp;rft.aufirst=Ruha&amp;rft.aulast=Benjamin&amp;rft.au=Ruha%20Benjamin&amp;rft.date=2019&amp;rft.isbn=978-1-5095-2643-7"></span>
  <div class="csl-entry">Chowdhury, Rumman. <i>Sharing Learnings about Our Image Cropping Algorithm</i>. 19 May 2021, <a href="https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm">https://blog.twitter.com/engineering/en_us/topics/insights/2021/sharing-learnings-about-our-image-cropping-algorithm</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Sharing%20learnings%20about%20our%20image%20cropping%20algorithm&amp;rft.description=Twitter%20shares%20a%20technical%20analysis%20of%20its%20assessment%20for%20potential%20bias%20in%20its%20image%20cropping%20algorithm%20as%20part%20of%20its%20efforts%20to%20be%20more%20transparent%20around%20how%20it%20uses%20machine%20learning%20to%20improve%20pe&amp;rft.identifier=https%3A%2F%2Fblog.twitter.com%2Fengineering%2Fen_us%2Ftopics%2Finsights%2F2021%2Fsharing-learnings-about-our-image-cropping-algorithm&amp;rft.aufirst=Rumman&amp;rft.aulast=Chowdhury&amp;rft.au=Rumman%20Chowdhury&amp;rft.date=2021-05-19&amp;rft.language=en_us"></span>
  <div class="csl-entry">Friedman, Batya, and Helen Nissenbaum. “Bias in Computer Systems.” <i>ACM Transactions on Information Systems</i>, vol. 14, no. 3, July 1996, pp. 330–47. <i>July 1996</i>, <a href="https://doi.org/10.1145/230538.230561">https://doi.org/10.1145/230538.230561</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F230538.230561&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Bias%20in%20computer%20systems&amp;rft.jtitle=ACM%20Transactions%20on%20Information%20Systems&amp;rft.stitle=ACM%20Trans.%20Inf.%20Syst.&amp;rft.volume=14&amp;rft.issue=3&amp;rft.aufirst=Batya&amp;rft.aulast=Friedman&amp;rft.au=Batya%20Friedman&amp;rft.au=Helen%20Nissenbaum&amp;rft.date=1996-07-01&amp;rft.pages=330%E2%80%93347&amp;rft.spage=330&amp;rft.epage=347&amp;rft.issn=1046-8188"></span>
  <div class="csl-entry">Vox. <i>Are We Automating Racism?</i> 2021. <i>YouTube</i>, <a href="https://www.youtube.com/watch?v=Ok5sKLXqynQ">https://www.youtube.com/watch?v=Ok5sKLXqynQ</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=videoRecording&amp;rft.title=Are%20We%20Automating%20Racism%3F&amp;rft.description=923%2C164%20views%20%E2%80%A2%20Mar%2031%2C%202021%20%E2%80%A2%20Many%20of%20us%20assume%20that%20tech%20is%20neutral%2C%20and%20we%20have%20turned%20to%20tech%20as%20a%20way%20to%20root%20out%20racism%2C%20sexism%2C%20or%20other%20%E2%80%9Cisms%E2%80%9D%20plaguing%20human%20decision-making.%20But%20as%20data-driven%20systems%20become%20a%20bigger%20and%20bigger%20part%20of%20our%20lives%2C%20we%20also%20notice%20more%20and%20more%20when%20they%20fail%2C%20and%2C%20more%20importantly%2C%20that%20they%20don%E2%80%99t%20fail%20on%20everyone%20equally.%20Glad%20You%20Asked%20host%20Joss%20Fong%20wants%20to%20know%3A%20Why%20do%20we%20think%20tech%20is%20neutral%3F%20How%20do%20algorithms%20become%20biased%3F%20And%20how%20can%20we%20fix%20these%20algorithms%20before%20they%20cause%20harm%3F%0A%20%20%20%20%20%20%0A%0A%0A%0A%0A%0A%20%20%0A%20%20%20%20%20%20%20%20Show%20less%0A%20%20%20%20%20%20%0A%0A%0A%20%20%0A%20%20%20%20%20%20%20%20Show%20more&amp;rft.identifier=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DOk5sKLXqynQ&amp;rft.aucorp=Vox&amp;rft.au=undefined&amp;rft.date=2021-03-31"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>1/13</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Feathers, Todd. <i>Police Are Telling ShotSpotter to Alter Evidence From Gunshot-Detecting AI</i>. 26 July 2021, <a href="https://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai">https://www.vice.com/en/article/qj8xbq/police-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Police%20Are%20Telling%20ShotSpotter%20to%20Alter%20Evidence%20From%20Gunshot-Detecting%20AI&amp;rft.description=Prosecutors%20in%20Chicago%20are%20being%20forced%20to%20withdraw%20evidence%20generated%20by%20the%20technology%2C%20which%20led%20to%20the%20police%20killing%20of%2013-year-old%20Adam%20Toledo%20earlier%20this%20year.&amp;rft.identifier=https%3A%2F%2Fwww.vice.com%2Fen%2Farticle%2Fqj8xbq%2Fpolice-are-telling-shotspotter-to-alter-evidence-from-gunshot-detecting-ai&amp;rft.aufirst=Todd&amp;rft.aulast=Feathers&amp;rft.au=Todd%20Feathers&amp;rft.date=2021-07-26&amp;rft.language=en"></span>
  <div class="csl-entry">Mitchell, Shira, et al. “Algorithmic Fairness: Choices, Assumptions, and Definitions.” <i>Annual Review of Statistics and Its Application</i>, vol. 8, no. 1, 2021, p. null. <i>Annual Reviews</i>, <a href="https://doi.org/10.1146/annurev-statistics-042720-125902">https://doi.org/10.1146/annurev-statistics-042720-125902</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1146%2Fannurev-statistics-042720-125902&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Algorithmic%20Fairness%3A%20Choices%2C%20Assumptions%2C%20and%20Definitions&amp;rft.jtitle=Annual%20Review%20of%20Statistics%20and%20Its%20Application&amp;rft.volume=8&amp;rft.issue=1&amp;rft.aufirst=Shira&amp;rft.aulast=Mitchell&amp;rft.au=Shira%20Mitchell&amp;rft.au=Eric%20Potash&amp;rft.au=Solon%20Barocas&amp;rft.au=Alexander%20D%E2%80%99Amour&amp;rft.au=Kristian%20Lum&amp;rft.date=2021&amp;rft.pages=null"></span>
  <div class="csl-entry">Smith, Ben. “How TikTok Reads Your Mind.” <i>The New York Times</i>, 6 Dec. 2021. <i>NYTimes.com</i>, <a href="https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html">https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=newspaperArticle&amp;rft.title=How%20TikTok%20Reads%20Your%20Mind&amp;rft.source=The%20New%20York%20Times&amp;rft.description=It%E2%80%99s%20the%20most%20successful%20video%20app%20in%20the%20world.%20Our%20columnist%20has%20obtained%20an%20internal%20company%20document%20that%20offers%20a%20new%20level%20of%20detail%20about%20how%20the%20algorithm%20works.&amp;rft.identifier=https%3A%2F%2Fwww.nytimes.com%2F2021%2F12%2F05%2Fbusiness%2Fmedia%2Ftiktok-algorithm.html&amp;rft.aufirst=Ben&amp;rft.aulast=Smith&amp;rft.au=Ben%20Smith&amp;rft.date=2021-12-06&amp;rft.issn=0362-4331&amp;rft.language=en-US"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>1/18</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Jeffries, Adrianne, et al. “Swinging the Vote?” <i>The Markup</i>, Feb. 2020, <a href="https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email">https://themarkup.org/google-the-giant/2020/02/26/wheres-my-email</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=Swinging%20the%20Vote%3F&amp;rft.source=The%20Markup&amp;rft.identifier=https%3A%2F%2Fthemarkup.org%2Fgoogle-the-giant%2F2020%2F02%2F26%2Fwheres-my-email&amp;rft.aufirst=Adrianne&amp;rft.aulast=Jeffries&amp;rft.au=Adrianne%20Jeffries&amp;rft.au=Leon%20Yin&amp;rft.au=Surya%20Mattu&amp;rft.date=2020-02-26&amp;rft.language=en"></span>
  <div class="csl-entry">Kirchner, Lauren. “Powerful DNA Software Used in Hundreds of Criminal Cases Faces New Scrutiny.” <i>The Markup</i>, Mar. 2021, <a href="https://themarkup.org/news/2021/03/09/powerful-dna-software-used-in-hundreds-of-criminal-cases-faces-new-scrutiny">https://themarkup.org/news/2021/03/09/powerful-dna-software-used-in-hundreds-of-criminal-cases-faces-new-scrutiny</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=Powerful%20DNA%20Software%20Used%20in%20Hundreds%20of%20Criminal%20Cases%20Faces%20New%20Scrutiny&amp;rft.source=The%20Markup&amp;rft.description=After%20decades%20of%20secrecy%2C%20two%20judges%20have%20ruled%20defendants%20can%20investigate%20whether%20TrueAllele%E2%80%99s%20probabilistic%20genotyping%20algorithm%20works%20as%20advertised&amp;rft.identifier=https%3A%2F%2Fthemarkup.org%2Fnews%2F2021%2F03%2F09%2Fpowerful-dna-software-used-in-hundreds-of-criminal-cases-faces-new-scrutiny&amp;rft.aufirst=Lauren&amp;rft.aulast=Kirchner&amp;rft.au=Lauren%20Kirchner&amp;rft.date=2021-03-09&amp;rft.language=en"></span>
  <div class="csl-entry">---. “When Zombie Data Costs You a Home.” <i>The Markup</i>, Oct. 2020, <a href="https://themarkup.org/locked-out/2020/10/06/zombie-criminal-records-housing-background-checks">https://themarkup.org/locked-out/2020/10/06/zombie-criminal-records-housing-background-checks</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=When%20Zombie%20Data%20Costs%20You%20a%20Home&amp;rft.source=The%20Markup&amp;rft.description=Even%20when%20people%20get%20their%20criminal%20records%20expunged%2C%20data%20brokers%20ensure%20the%20past%20keeps%20coming%20back%20to%20haunt%20them&amp;rft.identifier=https%3A%2F%2Fthemarkup.org%2Flocked-out%2F2020%2F10%2F06%2Fzombie-criminal-records-housing-background-checks&amp;rft.aufirst=Lauren&amp;rft.aulast=Kirchner&amp;rft.au=Lauren%20Kirchner&amp;rft.date=2020-10-06&amp;rft.language=en"></span>
  <div class="csl-entry">Varner, Maddy, and Aaron Sankin. “Suckers List: How Allstate’s Secret Auto Insurance Algorithm Squeezes Big Spenders.” <i>The Markup</i>, Feb. 2020, <a href="https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list">https://themarkup.org/allstates-algorithm/2020/02/25/car-insurance-suckers-list</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=Suckers%20List%3A%20How%20Allstate%E2%80%99s%20Secret%20Auto%20Insurance%20Algorithm%20Squeezes%20Big%20Spenders&amp;rft.source=The%20Markup&amp;rft.identifier=https%3A%2F%2Fthemarkup.org%2Fallstates-algorithm%2F2020%2F02%2F25%2Fcar-insurance-suckers-list&amp;rft.aufirst=Maddy&amp;rft.aulast=Varner&amp;rft.au=Maddy%20Varner&amp;rft.au=Aaron%20Sankin&amp;rft.date=2020-02-25&amp;rft.language=en"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>1/20</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Fry, Hannah. “What Statistics Can and Can’t Tell Us About Ourselves.” <i>The New Yorker</i>, Sept. 2019, <a href="https://www.newyorker.com/magazine/2019/09/09/what-statistics-can-and-cant-tell-us-about-ourselves">https://www.newyorker.com/magazine/2019/09/09/what-statistics-can-and-cant-tell-us-about-ourselves</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=What%20Statistics%20Can%20and%20Can%E2%80%99t%20Tell%20Us%20About%20Ourselves&amp;rft.source=The%20New%20Yorker&amp;rft.description=In%20the%20era%20of%20Big%20Data%2C%20we%E2%80%99ve%20come%20to%20believe%20that%2C%20with%20enough%20information%2C%20human%20behavior%20is%20predictable.%20But%20number%20crunching%20can%20lead%20us%20perilously%20wrong.&amp;rft.identifier=https%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2019%2F09%2F09%2Fwhat-statistics-can-and-cant-tell-us-about-ourselves&amp;rft.aufirst=Hannah&amp;rft.aulast=Fry&amp;rft.au=Hannah%20Fry&amp;rft.date=2019-09-02&amp;rft.language=en-us"></span>
  <div class="csl-entry">Ochigame, Rodrigo. “The Long History of Algorithmic Fairness.” <i>Phenomenal World</i>, 30 Jan. 2020, <a href="https://phenomenalworld.org/analysis/long-history-algorithmic-fairness">https://phenomenalworld.org/analysis/long-history-algorithmic-fairness</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=The%20Long%20History%20of%20Algorithmic%20Fairness&amp;rft.description=Fair%20algorithms%20from%20the%20seventeenth%20century%20to%20the%20present.&amp;rft.identifier=https%3A%2F%2Fphenomenalworld.org%2Fanalysis%2Flong-history-algorithmic-fairness&amp;rft.aufirst=Rodrigo&amp;rft.aulast=Ochigame&amp;rft.au=Rodrigo%20Ochigame&amp;rft.date=2020-01-30"></span>
  <div class="csl-entry">Onuoha, Mimi. “When Proof Is Not Enough.” <i>FiveThirtyEight</i>, 1 July 2020, <a href="https://fivethirtyeight.com/features/when-proof-is-not-enough/">https://fivethirtyeight.com/features/when-proof-is-not-enough/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=When%20Proof%20Is%20Not%20Enough&amp;rft.description=Throughout%20history%2C%20evidence%20of%20racism%20has%20failed%20to%20effect%20change.&amp;rft.identifier=https%3A%2F%2Ffivethirtyeight.com%2Ffeatures%2Fwhen-proof-is-not-enough%2F&amp;rft.aufirst=Mimi&amp;rft.aulast=Onuoha&amp;rft.au=Mimi%20Onuoha&amp;rft.date=2020-07-01&amp;rft.language=en-US"></span>
  <div class="csl-entry">Whitby, Andrew. “A Brief History of the Census—and How Covid-19 Could Change It.” <i>Wired</i>, Apr. 2020. <i>www.wired.com</i>, <a href="https://www.wired.com/story/brief-history-census-how-covid-19-could-change-it/">https://www.wired.com/story/brief-history-census-how-covid-19-could-change-it/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=A%20Brief%20History%20of%20the%20Census%E2%80%94and%20How%20Covid-19%20Could%20Change%20It&amp;rft.source=Wired&amp;rft.description=Census-taking%20used%20to%20spur%20innovation.%20Now%2C%20the%20pace%20of%20technology%2C%20and%20the%20challenge%20of%20going%20door-to-door%2C%20could%20force%20it%20to%20adopt%20existing%20systems%20instead.&amp;rft.identifier=https%3A%2F%2Fwww.wired.com%2Fstory%2Fbrief-history-census-how-covid-19-could-change-it%2F&amp;rft.aufirst=Andrew&amp;rft.aulast=Whitby&amp;rft.au=Andrew%20Whitby&amp;rft.date=2020-04-01&amp;rft.issn=1059-1028&amp;rft.language=en-us"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>1/25</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Buolamwini, Joy, et al. <i>Facial Recognition Technologies: A Primer</i>. 29 May 2020, <a href="https://global-uploads.webflow.com/5e027ca188c99e3515b404b7/5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf">https://global-uploads.webflow.com/5e027ca188c99e3515b404b7/5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=document&amp;rft.title=Facial%20Recognition%20Technologies%3A%20A%20Primer&amp;rft.identifier=https%3A%2F%2Fglobal-uploads.webflow.com%2F5e027ca188c99e3515b404b7%2F5ed1002058516c11edc66a14_FRTsPrimerMay2020.pdf&amp;rft.aufirst=Joy&amp;rft.aulast=Buolamwini&amp;rft.au=Joy%20Buolamwini&amp;rft.au=Vicente%20Ord%C3%B3%C3%B1ez&amp;rft.au=Jamie%20Morgenstern&amp;rft.au=Erik%20Learned-Miller&amp;rft.date=2020-05-29"></span>
  <div class="csl-entry">Hill, Kashmir, and Aaron Krolik. “How Photos of Your Kids Are Powering Surveillance Technology.” <i>The New York Times</i>, 11 Oct. 2019. <i>NYTimes.com</i>, <a href="https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html">https://www.nytimes.com/interactive/2019/10/11/technology/flickr-facial-recognition.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=newspaperArticle&amp;rft.title=How%20Photos%20of%20Your%20Kids%20Are%20Powering%20Surveillance%20Technology&amp;rft.source=The%20New%20York%20Times&amp;rft.description=Millions%20of%20Flickr%20images%20were%20sucked%20into%20a%20database%20called%20MegaFace.%20Now%20some%20of%20those%20faces%20may%20have%20the%20ability%20to%20sue.&amp;rft.identifier=https%3A%2F%2Fwww.nytimes.com%2Finteractive%2F2019%2F10%2F11%2Ftechnology%2Fflickr-facial-recognition.html&amp;rft.aufirst=Kashmir&amp;rft.aulast=Hill&amp;rft.au=Kashmir%20Hill&amp;rft.au=Aaron%20Krolik&amp;rft.date=2019-10-11&amp;rft.issn=0362-4331&amp;rft.language=en-US"></span>
  <div class="csl-entry">McVean, Ada. “40 Years of Human Experimentation in America: The Tuskegee Study.” <i>Office for Science and Society</i>, 25 Jan. 2019, <a href="https://www.mcgill.ca/oss/article/history/40-years-human-experimentation-america-tuskegee-study">https://www.mcgill.ca/oss/article/history/40-years-human-experimentation-america-tuskegee-study</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=40%20Years%20of%20Human%20Experimentation%20in%20America%3A%20The%20Tuskegee%20Study&amp;rft.description=Starting%20in%201932%2C%20600%20African%20American%20men%20from%20Macon%20County%2C%20Alabama%20were%20enlisted%20to%20partake%20in%20a%20scientific%20experiment%20on%20syphilis.%20The%20%E2%80%9CTuskegee%20Study%20of%20Untreated%20Syphilis%20in%20the%20Negro%20Male%2C%E2%80%9D%20was%20conducted%20by%20the%20United%20States%20Public%20Health%20Service%20(USPHS)%20and%20involved%20blood%20tests%2C%20x-rays%2C%20spinal%20taps%20and%20autopsies%20of%20the%20subjects.%20The%20goal%20was%20to%20%E2%80%9Cobserve%20the%20natural%20history%20of%20untreated%20syphilis%E2%80%9D%20in%20black%20populations.%20But%20the%20subjects%20were%20unaware%20of%20this%20and%20were%20simply%20told%20they%20were%20receiving%20treatment%20for%20bad%20blood.%20Actually%2C%20they%20received%20no%20treatment%20at%20all.%20Even%20after%20penicillin%20was%20discovered%20as%20a%20safe%20and%20reliable%20cure%20for%20syphilis%2C%20the%20majority%20of%20men%20did%20not%20receive%20it.%20To%20really%20understand%20the%20heinous%20nature%20of%20the%20Tuskegee%20Experiment%20requires%20some%20societal%20context%2C%20a%20lot%20of%20history%2C%20and%20a%20realization%20of%20just%20how%20many%20times%20government%20agencies%20were%20given%20a%20chance%20to%20stop%20this%20human%20experimentation%20but%20didn%E2%80%99t.%20In%201865%2C%20the%20ratification%20of%20the%20Thirteenth%20Amendment%20of%20the%20U.S.%20Constitution%20formally%20ended%20the%20enslavement%20of%20black%20Americans.%20But%20by%20the%20early%2020th%20century%2C%20the%20cultural%20and%20medical%20landscape%20of%20the%20U.S.%20was%20still%20built%20upon%20and%20inundated%20with%20racist%20concepts.%20Social%20Darwinism%20was%20rising%2C%20predicated%20on%20the%20survival%20of%20the%20fittest%2C%20and%20%E2%80%9Cscientific%20racism%E2%80%9D%20(a%20pseudoscientific%20practice%20of%20using%20science%20to%20reinforce%20racial%20biases)%20was%20common.%20Many%20white%20people%20already%20thought%20themselves%20superior%20to%20blacks%20and%20science%20and%20medicine%20was%20all%20too%20happy%20to%20reinforce%20this%20hierarchy.%20Before%20the%20ending%20of%20slavery%2C%20scientific%20racism%20was%20used%20to%20justify%20the%20African%20slave%20trade.%20Scientists%20argued%20that%20African%20men%20were%20uniquely%20fit%20for%20enslavement%20due%20to%20their%20physical%20strength%20and%20simple%20minds.%20They%20argued%20that%20slaves%20possessed%20primitive%20nervous%20systems%2C%20so%20did%20not%20experience%20pain%20as%20white%20people%20did.%20Enslaved%20African%20Americans%20in%20the%20South%20were%20claimed%20to%20suffer%20from%20mental%20illness%20at%20rates%20lower%20than%20their%20free%20Northern%20counterparts%20(thereby%20proving%20that%20enslavement%20was%20good%20for%20them)%2C%20and%20slaves%20who%20ran%20away%20were%20said%20to%20be%20suffering%20from%20their%20own%20mental%20illness%20known%20as%20drapetomania.%20During%20and%20after%20the%20American%20Civil%20War%2C%20African%20Americans%20were%20argued%20to%20be%20a%20different%20species%20from%20white%20Americans%2C%20and%20mixed-race%20children%20were%20presumed%20prone%20to%20many%20medical%20issues.%20Doctors%20of%20the%20time%20testified%20that%20the%20emancipation%20of%20slaves%20had%20caused%20the%20%E2%80%9Cmental%2C%20moral%20and%20physical%20deterioration%20of%20the%20black%20population%2C%E2%80%9D%20observing%20that%20%E2%80%9Cvirtually%20free%20of%20disease%20as%20slaves%2C%20they%20were%20now%20overwhelmed%20by%20it.%E2%80%9D%20Many%20believed%20that%20the%20African%20Americans%20were%20doomed%20to%20extinction%2C%20and%20arguments%20were%20made%20about%20their%20physiology%20being%20unsuited%20for%20the%20colder%20climates%20of%20America%20(thus%20they%20should%20be%20returned%20to%20Africa).%20Scientific%20and%20medical%20authorities%20of%20the%20late%2019th%2Fearly%2020th%20centuries%20held%20extremely%20harmful%20pseudoscientific%20ideas%20specifically%20about%20the%20sex%20drives%20and%20genitals%20of%20African%20Americans.%20It%20was%20widely%20believed%20that%2C%20while%20the%20brains%20of%20African%20Americans%20were%20under-evolved%2C%20their%20genitals%20were%20over-developed.%20Black%20men%20were%20seen%20to%20have%20an%20intrinsic%20perversion%20for%20white%20women%2C%20and%20all%20African%20Americans%20were%20seen%20as%20inherently%20immoral%2C%20with%20insatiable%20sexual%20appetites.%20This%20all%20matters%20because%20it%20was%20with%20these%20understandings%20of%20race%2C%20sexuality%20and%20health%20that%20researchers%20undertook%20the%20Tuskegee%20study.%20They%20believed%2C%20largely%20due%20to%20their%20fundamentally%20flawed%20scientific%20understandings%20of%20race%2C%20that%20black%20people%20were%20extremely%20prone%20to%20sexually%20transmitted%20infections%20(like%20syphilis).%20Low%20birth%20rates%20and%20high%20miscarriage%20rates%20were%20universally%20blamed%20on%20STIs.%20They%20also%20believed%20that%20all%20black%20people%2C%20regardless%20of%20their%20education%2C%20background%2C%20economic%20or%20personal%20situations%2C%20could%20not%20be%20convinced%20to%20get%20treatment%20for%20syphilis.%20Thus%2C%20the%20USPHS%20could%20justify%20the%20Tuskegee%20study%2C%20calling%20it%20a%20%E2%80%9Cstudy%20in%20nature%E2%80%9D%20rather%20than%20an%20experiment%2C%20meant%20to%20simply%20observe%20the%20natural%20progression%20of%20syphilis%20within%20a%20community%20that%20wouldn%E2%80%99t%20seek%20treatment.%20The%20USPHS%20set%20their%20study%20in%20Macon%20County%20due%20to%20estimates%20that%2035%25%20of%20its%20population%20was%20infected%20with%20syphilis.%20In%201932%2C%20the%20initial%20patients%20between%20the%20ages%20of%2025%20and%2060%20were%20recruited%20under%20the%20guise%20of%20receiving%20free%20medical%20care%20for%20%E2%80%9Cbad%20blood%2C%E2%80%9D%20a%20colloquial%20term%20encompassing%20anemia%2C%20syphilis%2C%20fatigue%20and%20other%20conditions.%20Told%20that%20the%20treatment%20would%20last%20only%20six%20months%2C%20they%20received%20physical%20examinations%2C%20x-rays%2C%20spinal%20taps%2C%20and%20when%20they%20died%2C%20autopsies.%20Researchers%20faced%20a%20lack%20of%20participants%20due%20to%20fears%20that%20the%20physical%20examinations%20were%20actually%20for%20the%20purpose%20of%20recruiting%20them%20to%20the%20military.%20To%20assuage%20these%20fears%2C%20doctors%20began%20examining%20women%20and%20children%20as%20well.%20Men%20diagnosed%20with%20syphilis%20who%20were%20of%20the%20appropriate%20age%20were%20recruited%20for%20the%20study%2C%20while%20others%20received%20proper%20treatments%20for%20their%20syphilis%20(at%20the%20time%20these%20were%20commonly%20mercury-%20or%20arsenic-containing%20medicines).%20In%201933%2C%20researchers%20decided%20to%20continue%20the%20study%20long%20term.%20They%20recruited%20200%2B%20control%20patients%20who%20did%20not%20have%20syphilis%20(simply%20switching%20them%20to%20the%20syphilis-positive%20group%20if%20at%20any%20time%20they%20developed%20it).%20They%20also%20began%20giving%20all%20patients%20ineffective%20medicines%20(%20ointments%20or%20capsules%20with%20too%20small%20doses%20of%20neoarsphenamine%20or%20mercury)%20to%20further%20their%20belief%20that%20they%20were%20being%20treated.%20As%20time%20progressed%2C%20however%2C%20patients%20began%20to%20stop%20attending%20their%20appointments.%20To%20greater%20incentivize%20them%20to%20remain%20a%20part%20of%20the%20study%2C%20the%20USPHS%20hired%20a%20nurse%20named%20Eunice%20Rivers%20to%20drive%20them%20to%20and%20from%20their%20appointments%2C%20provide%20them%20with%20hot%20meals%20and%20deliver%20their%20medicines%2C%20services%20especially%20valuable%20to%20subjects%20during%20the%20Great%20Depression.%20In%20an%20effort%20to%20ensure%20the%20autopsies%20of%20their%20test%20subjects%2C%20the%20researchers%20also%20began%20covering%20patient%E2%80%99s%20funeral%20expenses.%20Multiple%20times%20throughout%20the%20experiment%20researchers%20actively%20worked%20to%20ensure%20that%20their%20subjects%20did%20not%20receive%20treatment%20for%20syphilis.%20In%201934%20they%20provided%20doctors%20in%20Macon%20County%20with%20lists%20of%20their%20subjects%20and%20asked%20them%20not%20to%20treat%20them.%20In%201940%20they%20did%20the%20same%20with%20the%20Alabama%20Health%20Department.%20In%201941%20many%20of%20the%20men%20were%20drafted%20and%20had%20their%20syphilis%20uncovered%20by%20the%20entrance%20medical%20exam%2C%20so%20the%20researchers%20had%20the%20men%20removed%20from%20the%20army%2C%20rather%20than%20let%20their%20syphilis%20be%20treated.%20It%20was%20in%20these%20moments%20that%20the%20Tuskegee%20study%E2%80%99s%20true%20nature%20became%20clear.%20Rather%20than%20simply%20observing%20and%20documenting%20the%20natural%20progression%20of%20syphilis%20in%20the%20community%20as%20had%20been%20planned%2C%20the%20researchers%20intervened%3A%20first%20by%20telling%20the%20participants%20that%20they%20were%20being%20treated%20(a%20lie)%2C%20and%20then%20again%20by%20preventing%20their%20participants%20from%20seeking%20treatment%20that%20could%20save%20their%20lives.%20Thus%2C%20the%20original%20basis%20for%20the%20study--that%20the%20people%20of%20Macon%20County%20would%20likely%20not%20seek%20treatment%20and%20thus%20could%20be%20observed%20as%20their%20syphilis%20progressed--became%20a%20self-fulfilling%20prophecy.%20The%20Henderson%20Act%20was%20passed%20in%201943%2C%20requiring%20tests%20and%20treatments%20for%20venereal%20diseases%20to%20be%20publicly%20funded%2C%20and%20by%201947%2C%20penicillin%20had%20become%20the%20standard%20treatment%20for%20syphilis%2C%20prompting%20the%20USPHS%20to%20open%20several%20Rapid%20Treatment%20Centers%20specifically%20to%20treat%20syphilis%20with%20penicillin.%20All%20the%20while%20they%20were%20actively%20preventing%20399%20men%20from%20receiving%20the%20same%20treatments.%20By%201952%2C%20however%2C%20about%2030%25%20of%20the%20participants%20had%20received%20penicillin%20anyway%2C%20despite%20the%20researchers%E2%80%99%20best%20efforts.%20Regardless%2C%20the%20USPHS%20argued%20that%20their%20participants%20wouldn%E2%80%99t%20seek%20penicillin%20or%20stick%20to%20the%20prescribed%20treatment%20plans.%20They%20claimed%20that%20their%20participants%2C%20all%20black%20men%2C%20were%20too%20%E2%80%9Cstoic%E2%80%9D%20to%20visit%20a%20doctor.%20In%20truth%20these%20men%20thought%20they%20were%20already%20being%20treated%2C%20so%20why%20would%20they%20seek%20out%20further%20treatment%3F%20The%20researchers%E2%80%99%20tune%20changed%20again%20as%20time%20went%20on.%20In%201965%2C%20they%20argued%20that%20it%20was%20too%20late%20to%20give%20the%20subjects%20penicillin%2C%20as%20their%20syphilis%20had%20progressed%20too%20far%20for%20the%20drug%20to%20help.%20While%20a%20convenient%20justification%20for%20their%20continuation%20of%20the%20study%2C%20penicillin%20is%20(and%20was)%20recommended%20for%20all%20stages%20of%20syphilis%20and%20could%20have%20stopped%20the%20disease%E2%80%99s%20progression%20in%20the%20patients.%20In%201947%20the%20Nuremberg%20code%20was%20written%2C%20and%20in%201964%20the%20World%20Health%20Organization%20published%20their%20Declaration%20of%20Helsinki.%20Both%20aimed%20to%20protect%20humans%20from%20experimentation%2C%20but%20despite%20this%2C%20the%20Centers%20for%20Disease%20Control%20(which%20had%20taken%20over%20from%20the%20USPHS%20in%20controlling%20the%20study)%20actively%20decided%20to%20continue%20the%20study%20as%20late%20as%201969.%20It%20wasn%E2%80%99t%20until%20a%20whistleblower%2C%20Peter%20Buxtun%2C%20leaked%20information%20about%20the%20study%20to%20the%20New%20York%20Times%20and%20the%20paper%20published%20it%20on%20the%20front%20page%20on%20November%2016th%2C%201972%2C%20that%20the%20Tuskegee%20study%20finally%20ended.%20By%20this%20time%20only%2074%20of%20the%20test%20subjects%20were%20still%20alive.%20128%20patients%20had%20died%20of%20syphilis%20or%20its%20complications%2C%2040%20of%20their%20wives%20had%20been%20infected%2C%20and%2019%20of%20their%20children%20had%20acquired%20congenital%20syphilis.%20There%20was%20mass%20public%20outrage%2C%20and%20the%20National%20Association%20for%20the%20Advancement%20of%20Colored%20People%20launched%20a%20class%20action%20lawsuit%20against%20the%20USPHS.%20It%20settled%20the%20suit%20two%20years%20later%20for%2010%20million%20dollars%20and%20agreed%20to%20pay%20the%20medical%20treatments%20of%20all%20surviving%20participants%20and%20infected%20family%20members%2C%20the%20last%20of%20whom%20died%20in%202009.%20Largely%20in%20response%20to%20the%20Tuskegee%20study%2C%20Congress%20passed%20the%20National%20Research%20Act%20in%201974%2C%20and%20the%20Office%20for%20Human%20Research%20Protections%20was%20established%20within%20the%20USPHS.%20Obtaining%20informed%20consent%20from%20all%20study%20participants%20became%20required%20for%20all%20research%20on%20humans%2C%20with%20this%20process%20overseen%20by%20Institutional%20Review%20Boards%20(IRBs)%20within%20academia%20and%20hospitals.%20The%20Tuskegee%20study%20has%20had%20lasting%20effects%20on%20America.%20It%E2%80%99s%20estimated%20that%20the%20life%20expectancy%20of%20black%20men%20fell%20by%20up%20to%201.4%20years%20when%20the%20study%E2%80%99s%20details%20came%20to%20light.%20Many%20also%20blame%20the%20study%20for%20impacting%20the%20willingness%20of%20black%20individuals%20to%20willingly%20participate%20in%20medical%20research%20today.%20We%20know%20all%20about%20evil%20Nazis%20who%20experimented%20on%20prisoners.%20We%20condemn%20the%20scientists%20in%20Marvel%20movies%20who%20carry%20out%20tests%20on%20prisoners%20of%20war.%20But%20we%E2%80%99d%20do%20well%20to%20remember%20that%20America%20has%20also%20used%20its%20own%20people%20as%20lab%20rats.%20Yet%20to%20this%20day%2C%20no%20one%20has%20been%20prosecuted%20for%20their%20role%20in%20dooming%20399%20men%20to%20syphilis.%20%40Adamcvean%20Want%20to%20comment%20on%20this%20article%3F%20View%20it%20on%20our%20Facebook%20page!&amp;rft.identifier=https%3A%2F%2Fwww.mcgill.ca%2Foss%2Farticle%2Fhistory%2F40-years-human-experimentation-america-tuskegee-study&amp;rft.aufirst=Ada&amp;rft.aulast=McVean&amp;rft.au=Ada%20McVean&amp;rft.date=2019-01-25&amp;rft.language=en"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>1/27</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Buolamwini, Joy, and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” <i>Conference on Fairness, Accountability and Transparency</i>, PMLR, 2018, pp. 77–91. <i>proceedings.mlr.press</i>, <a href="http://proceedings.mlr.press/v81/buolamwini18a.html">http://proceedings.mlr.press/v81/buolamwini18a.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Gender%20Shades%3A%20Intersectional%20Accuracy%20Disparities%20in%20Commercial%20Gender%20Classification&amp;rft.btitle=Conference%20on%20Fairness%2C%20Accountability%20and%20Transparency&amp;rft.publisher=PMLR&amp;rft.aufirst=Joy&amp;rft.aulast=Buolamwini&amp;rft.au=Joy%20Buolamwini&amp;rft.au=Timnit%20Gebru&amp;rft.date=2018-01-21&amp;rft.pages=77-91&amp;rft.spage=77&amp;rft.epage=91&amp;rft.language=en"></span>
  <div class="csl-entry">Castelvecchi, Davide. “Is Facial Recognition Too Biased to Be Let Loose?” <i>Nature</i>, vol. 587, no. 7834, 7834, Nature Publishing Group, Nov. 2020, pp. 347–49. <i>www.nature.com</i>, <a href="https://doi.org/10.1038/d41586-020-03186-4">https://doi.org/10.1038/d41586-020-03186-4</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-020-03186-4&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Is%20facial%20recognition%20too%20biased%20to%20be%20let%20loose%3F&amp;rft.jtitle=Nature&amp;rft.volume=587&amp;rft.issue=7834&amp;rft.aufirst=Davide&amp;rft.aulast=Castelvecchi&amp;rft.au=Davide%20Castelvecchi&amp;rft.date=2020-11-18&amp;rft.pages=347-349&amp;rft.spage=347&amp;rft.epage=349&amp;rft.language=en"></span>
  <div class="csl-entry">MIT Media Lab. <i>Gender Shades</i>. 2018. <i>YouTube</i>, <a href="https://www.youtube.com/watch?v=TWWsW1w-BVo">https://www.youtube.com/watch?v=TWWsW1w-BVo</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=videoRecording&amp;rft.title=Gender%20Shades&amp;rft.description=The%20Gender%20Shades%20Project%20pilots%20an%20intersectional%20approach%20to%20inclusive%20product%20testing%20for%20AI.%0A%0AGender%20Shades%20is%20a%20preliminary%20excavation%20of%20inadvertent%20negligence%20that%20will%20cripple%20the%20age%20of%20automation%20and%20further%20exacerbate%20inequality%20if%20left%20to%20fester.%20The%20deeper%20we%20dig%2C%20the%20more%20remnants%20of%20bias%20we%20will%20find%20in%20our%20technology.%20We%20cannot%20afford%20to%20look%20away%20this%20time%2C%20because%20the%20stakes%20are%20simply%20too%20high.%20%20We%20risk%20losing%20the%20gains%20made%20with%20the%20civil%20rights%20movement%20and%20women's%20movement%20under%20the%20false%20assumption%20of%20machine%20neutrality.%20Automated%20systems%20are%20not%20inherently%20neutral.%20They%20reflect%20the%20priorities%2C%20preferences%2C%20and%20prejudices%E2%80%94the%20coded%20gaze%E2%80%94of%20those%20who%20have%20the%20power%20to%20mold%20artificial%20intelligence.%0A%0AVideo%20produced%20by%20Joy%20Buolamwini%20and%20Jimmy%20Day%0A%0AMany%20thanks%20to%20the%20Natural%20Sciences%20and%20Engineering%20Research%20Council%20of%20Canada%20%7C%20Conseil%20de%20recherches%20en%20sciences%20naturelles%20et%20en%20g%C3%A9nie%20du%20Canada%20for%20translating%20the%20captions%20into%20French.%20%0A%0AMore%20information%20at%3A%20https%3A%2F%2Fwww.media.mit.edu%2Fprojects%2Fge...%E2%80%8B%20%0ALicense%3A%20Creative%20Commons%20Attribution-NonCommercial-NoDerivatives%204.0%20International%20Public%20License%20(https%3A%2F%2Fcreativecommons.org%2Flicenses%2F...%E2%80%8B)&amp;rft.identifier=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DTWWsW1w-BVo&amp;rft.aucorp=MIT%20Media%20Lab&amp;rft.au=undefined&amp;rft.date=2018-02-09"></span>
  <div class="csl-entry">Noorden, Richard Van. “The Ethical Questions That Haunt Facial-Recognition Research.” <i>Nature</i>, vol. 587, no. 7834, 7834, Nature Publishing Group, Nov. 2020, pp. 354–58. <i>www.nature.com</i>, <a href="https://doi.org/10.1038/d41586-020-03187-3">https://doi.org/10.1038/d41586-020-03187-3</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-020-03187-3&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The%20ethical%20questions%20that%20haunt%20facial-recognition%20research&amp;rft.jtitle=Nature&amp;rft.volume=587&amp;rft.issue=7834&amp;rft.aufirst=Richard%20Van&amp;rft.aulast=Noorden&amp;rft.au=Richard%20Van%20Noorden&amp;rft.date=2020-11-18&amp;rft.pages=354-358&amp;rft.spage=354&amp;rft.epage=358&amp;rft.language=en"></span>
</div></td>
    <td></td>
  </tr>  
  <tr>
    <td>2/1</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Angwin, Julia, et al. “Machine Bias.” <i>ProPublica</i>, 23 May 2016, <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing?token=jRdnwabwdw5HLiHY-R3nqWS5DOjEM7W-">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing?token=jRdnwabwdw5HLiHY-R3nqWS5DOjEM7W-</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Machine%20Bias&amp;rft.description=There%E2%80%99s%20software%20used%20across%20the%20country%20to%20predict%20future%20criminals.%20And%20it%E2%80%99s%20biased%20against%20blacks.&amp;rft.identifier=https%3A%2F%2Fwww.propublica.org%2Farticle%2Fmachine-bias-risk-assessments-in-criminal-sentencing%3Ftoken%3DjRdnwabwdw5HLiHY-R3nqWS5DOjEM7W-&amp;rft.aufirst=Julia&amp;rft.aulast=Angwin&amp;rft.au=Julia%20Angwin&amp;rft.au=Jeff%20Larson&amp;rft.au=Surya%20Mattu&amp;rft.au=Lauren%20Kirchner&amp;rft.date=2016-05-23&amp;rft.language=en"></span>
  <div class="csl-entry">Hill, Kashmir. “Wrongfully Accused by an Algorithm.” <i>The New York Times</i>, 24 June 2020. <i>NYTimes.com</i>, <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html">https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=newspaperArticle&amp;rft.title=Wrongfully%20Accused%20by%20an%20Algorithm&amp;rft.source=The%20New%20York%20Times&amp;rft.description=In%20what%20may%20be%20the%20first%20known%20case%20of%20its%20kind%2C%20a%20faulty%20facial%20recognition%20match%20led%20to%20a%20Michigan%20man%E2%80%99s%20arrest%20for%20a%20crime%20he%20did%20not%20commit.&amp;rft.identifier=https%3A%2F%2Fwww.nytimes.com%2F2020%2F06%2F24%2Ftechnology%2Ffacial-recognition-arrest.html&amp;rft.aufirst=Kashmir&amp;rft.aulast=Hill&amp;rft.au=Kashmir%20Hill&amp;rft.date=2020-06-24&amp;rft.issn=0362-4331&amp;rft.language=en-US"></span>
  <div class="csl-entry">Lum, Kristian, and William Isaac. “To Predict and Serve?” <i>Significance</i>, vol. 13, no. 5, 2016, pp. 14–19. <i>Wiley Online Library</i>, <a href="https://doi.org/10.1111/j.1740-9713.2016.00960.x">https://doi.org/10.1111/j.1740-9713.2016.00960.x</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2Fhttps%3A%2F%2Fdoi.org%2F10.1111%2Fj.1740-9713.2016.00960.x&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=To%20predict%20and%20serve%3F&amp;rft.jtitle=Significance&amp;rft.volume=13&amp;rft.issue=5&amp;rft.aufirst=Kristian&amp;rft.aulast=Lum&amp;rft.au=Kristian%20Lum&amp;rft.au=William%20Isaac&amp;rft.date=2016&amp;rft.pages=14-19&amp;rft.spage=14&amp;rft.epage=19&amp;rft.issn=1740-9713"></span>
  <div class="csl-entry">O’Neill, James. “How Facial Recognition Makes You Safer.” <i>The New York Times</i>, 9 June 2019. <i>NYTimes.com</i>, <a href="https://www.nytimes.com/2019/06/09/opinion/facial-recognition-police-new-york-city.html">https://www.nytimes.com/2019/06/09/opinion/facial-recognition-police-new-york-city.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=newspaperArticle&amp;rft.title=How%20Facial%20Recognition%20Makes%20You%20Safer&amp;rft.source=The%20New%20York%20Times&amp;rft.description=Used%20properly%2C%20the%20software%20effectively%20identifies%20crime%20suspects%20without%20violating%20rights.&amp;rft.identifier=https%3A%2F%2Fwww.nytimes.com%2F2019%2F06%2F09%2Fopinion%2Ffacial-recognition-police-new-york-city.html&amp;rft.aufirst=James&amp;rft.aulast=O%E2%80%99Neill&amp;rft.au=James%20O%E2%80%99Neill&amp;rft.date=2019-06-09&amp;rft.issn=0362-4331&amp;rft.language=en-US"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>2/3</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Aslanian, Sasha, and Jill Barshay. “Under a Watchful Eye.” <i>APM Reports</i>, 6 Aug. 2019, <a href="https://www.apmreports.org/episode/2019/08/06/college-data-tracking-students-graduation">https://www.apmreports.org/episode/2019/08/06/college-data-tracking-students-graduation</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Under%20a%20Watchful%20Eye&amp;rft.description=At%20Georgia%20State%20in%20Atlanta%2C%20more%20students%20are%20graduating%2C%20and%20the%20school%20credits%20its%20use%20of%20predictive%20analytics.%20But%20critics%20worry%20that%20the%20algorithms%20may%20be%20invading%20students'%20privacy%20and%20reinforcing%20racial%20inequities.&amp;rft.identifier=https%3A%2F%2Fwww.apmreports.org%2Fepisode%2F2019%2F08%2F06%2Fcollege-data-tracking-students-graduation&amp;rft.aufirst=Sasha&amp;rft.aulast=Aslanian&amp;rft.au=Sasha%20Aslanian&amp;rft.au=Jill%20Barshay&amp;rft.date=2019-08-06"></span>
  <div class="csl-entry">Ekowo, Manuela, and Iris Palmer. <i>The Promise and Peril of Predictive Analytics in Higher Education</i>. Oct. 2016, p. 36.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The%20Promise%20and%20Peril%20of%20Predictive%20Analytics%20in%20Higher%20Education&amp;rft.aufirst=Manuela&amp;rft.aulast=Ekowo&amp;rft.au=Manuela%20Ekowo&amp;rft.au=Iris%20Palmer&amp;rft.date=2016-10&amp;rft.pages=36&amp;rft.language=en"></span>
  <div class="csl-entry">Todd Feathers. <i>Major Universities Are Using Race as a “High Impact Predictor” of Student Success</i>. 2 Mar. 2021, <a href="https://themarkup.org/news/2021/03/02/major-universities-are-using-race-as-a-high-impact-predictor-of-student-success">https://themarkup.org/news/2021/03/02/major-universities-are-using-race-as-a-high-impact-predictor-of-student-success</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Major%20Universities%20Are%20Using%20Race%20as%20a%20%E2%80%9CHigh%20Impact%20Predictor%E2%80%9D%20of%20Student%20Success&amp;rft.description=Students%2C%20professors%2C%20and%20education%20experts%20worry%20that%20that%E2%80%99s%20pushing%20Black%20students%20in%20particular%20out%20of%20math%20and%20science&amp;rft.identifier=https%3A%2F%2Fthemarkup.org%2Fnews%2F2021%2F03%2F02%2Fmajor-universities-are-using-race-as-a-high-impact-predictor-of-student-success&amp;rft.aucorp=Todd%20Feathers&amp;rft.au=undefined&amp;rft.date=2021-03-02&amp;rft.language=en"></span>
</div></td>
    <td></td>
  </tr>  
  <tr>
    <td>2/8</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Crawford, Kate. “State.” <i>The Atlas of AI</i>, Yale University Press, 2021, pp. 181–209. <i>JSTOR</i>, <a href="https://doi.org/10.2307/j.ctv1ghv45t.9">https://doi.org/10.2307/j.ctv1ghv45t.9</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-0-300-20957-0&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=State&amp;rft.publisher=Yale%20University%20Press&amp;rft.series=Power%2C%20Politics%2C%20and%20the%20Planetary%20Costs%20of%20Artificial%20Intelligence&amp;rft.aufirst=Kate&amp;rft.aulast=Crawford&amp;rft.au=Kate%20Crawford&amp;rft.date=2021&amp;rft.pages=181-209&amp;rft.spage=181&amp;rft.epage=209&amp;rft.isbn=978-0-300-20957-0"></span>
  <div class="csl-entry">Green, Ben. <i>The Innovative City: The Relationship between Technical and Nontechnical Change in City Government</i>. 2019. <i>direct.mit.edu</i>, <a href="https://direct.mit.edu/books/book/4204/chapter/172388/The-Innovative-City-The-Relationship-between">https://direct.mit.edu/books/book/4204/chapter/172388/The-Innovative-City-The-Relationship-between</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=The%20Innovative%20City%3A%20The%20Relationship%20between%20Technical%20and%20Nontechnical%20Change%20in%20City%20Government&amp;rft.aufirst=Ben&amp;rft.aulast=Green&amp;rft.au=Ben%20Green&amp;rft.date=2019-04-07&amp;rft.language=en"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>2/10</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Ingold, David, and Spencer Soper. “Amazon Doesn’t Consider the Race of Its Customers. Should It?” <i>Bloomberg.Com</i>, 21 Apr. 2016, <a href="http://www.bloomberg.com/graphics/2016-amazon-same-day/">http://www.bloomberg.com/graphics/2016-amazon-same-day/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Amazon%20Doesn%E2%80%99t%20Consider%20the%20Race%20of%20Its%20Customers.%20Should%20It%3F&amp;rft.description=In%20six%20big%20cities%2C%20Amazon%20Prime%20Same-Day%20delivery%20doesn't%20serve%20black%20ZIP%20codes%20as%20well%20as%20it%20does%20white%20ones.&amp;rft.identifier=http%3A%2F%2Fwww.bloomberg.com%2Fgraphics%2F2016-amazon-same-day%2F&amp;rft.aufirst=David&amp;rft.aulast=Ingold&amp;rft.au=David%20Ingold&amp;rft.au=Spencer%20Soper&amp;rft.date=2016-04-21&amp;rft.language=en"></span>
  <div class="csl-entry">MIT Technology Review. <i>Podcast: In Machines We Trust - Hired by an Algorithm</i>. 2021. <i>YouTube</i>, <a href="https://www.youtube.com/watch?v=ztcVB_zh_M0">https://www.youtube.com/watch?v=ztcVB_zh_M0</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=videoRecording&amp;rft.title=Podcast%3A%20In%20Machines%20We%20Trust%20-%20Hired%20by%20an%20algorithm&amp;rft.description=If%20you%E2%80%99ve%20applied%20for%20a%20job%20lately%2C%20it%E2%80%99s%20all%20but%20guaranteed%20that%20your%20application%20was%20reviewed%20by%20software%E2%80%94in%20most%20cases%2C%20before%20a%20human%20ever%20laid%20eyes%20on%20it.%20In%20this%20episode%2C%20the%20first%20in%20a%20four-part%20investigation%20into%20automated%20hiring%20practices%2C%20we%20speak%20with%20the%20CEOs%20of%20ZipRecruiter%20and%20Career%20Builder%2C%20and%20one%20of%20the%20architects%20of%20LinkedIn%E2%80%99s%20algorithmic%20job-matching%20system%2C%20to%20explore%20how%20AI%20is%20increasingly%20playing%20matchmaker%20between%20job%20searchers%20and%20employers.%20But%20while%20software%20helps%20speed%20up%20the%20process%20of%20sifting%20through%20the%20job%20market%2C%20algorithms%20have%20a%20history%20of%20biasing%20the%20opportunities%20they%20present%20to%20people%20by%20gender%2C%20race...and%20in%20at%20least%20one%20case%2C%20whether%20you%20played%20lacrosse%20in%20high%20school.%0A%0APart%202%3A%20https%3A%2F%2Fyoutu.be%2FBAnU4yq0Qzo%0APart%203%3A%20https%3A%2F%2Fyoutu.be%2FhX8fSu9otNs%0A%0AWe%20Meet%3A%0AMark%20Girouard%2C%20Attorney%2C%20Nilan%20Johnson%20Lewis%0AIan%20Siegel%2C%20CEO%2C%20ZipRecruiter%0AJohn%20Jersin%2C%20former%20Vice%20President%20of%20Product%20Management%2C%20LinkedIn%0AIrina%20Novoselsky%2C%20CEO%2C%20CareerBuilder%0A%0AWe%20Talked%20To%3A%0AMark%20Girouard%2C%20Attorney%2C%20Nilan%20Johnson%20Lewis%0AIan%20Siegel%2C%20CEO%2C%20ZipRecruiter%0AJohn%20Jersin%2C%20former%20Vice%20President%20of%20Product%20Management%2C%20LinkedIn%0AIrina%20Novoselsky%2C%20CEO%2C%20CareerBuilder%0ADerek%20Kan%2C%20Vice%20President%20of%20Product%20Management%2C%20Monster%0AAleksandra%20Korolova%2C%20Assistant%20Professor%20of%20Computer%20Science%2C%20University%20of%20Southern%20California%0ABrian%20Kropp%2C%20Vice%20President%20Research%2C%20Gartner%0AMatthew%20Neale%2C%20Vice%20President%20of%20Assessment%20Products%2C%20Criteria%20Corp%0AJosh%20Bersin%2C%20Research%20Analyst%0AJonathan%20Kestenbaum%2C%20Co-Founder%20and%20Managing%20Director%2C%20Talent%20Tech%20Labs%0ASuresh%20Venkatasubramanian%2C%20Assistant%20Director%2C%20White%20House%20Office%20of%20Science%20and%20Technology%20Policy%0A%0ASounds%20From%3A%0AHow%20to%20Keep%20a%20Job%2C%20Coronet%20Instructional%20Films%3A%20https%3A%2F%2Farchive.org%2Fdetails%2FHowtoKee1949%0ACurious%20Thing%20AI%20(Sound%20from%20their%20AI%20tool)%0A%0ACredits%3A%0AThis%20episode%20was%20reported%20by%20Hilke%20Schellmann%2C%20and%20produced%20by%20Jennifer%20Strong%2C%20Emma%20Cillekens%20and%20Anthony%20Green%20with%20special%20thanks%20to%20Karen%20Hao.%20We%E2%80%99re%20edited%20by%20Michael%20Reilly.%0A%0AAdditional%20reporting%20from%20us%3A%0Ahttps%3A%2F%2Fwww.technologyreview.com%2F2021...%0Ahttps%3A%2F%2Fwww.technologyreview.com%2F2021...%0Ahttps%3A%2F%2Fwww.technologyreview.com%2F2021...%0Ahttps%3A%2F%2Fwww.technologyreview.com%2F2019...%0Ahttps%3A%2F%2Fwww.technologyreview.com%2F2020...&amp;rft.identifier=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DztcVB_zh_M0&amp;rft.aucorp=MIT%20Technology%20Review&amp;rft.au=undefined&amp;rft.date=2021-06-23"></span>
  <div class="csl-entry">Raghavan, Manish, and Solon Barocas. “Challenges for Mitigating Bias in Algorithmic Hiring.” <i>Brookings</i>, 6 Dec. 2019, <a href="https://www.brookings.edu/research/challenges-for-mitigating-bias-in-algorithmic-hiring/">https://www.brookings.edu/research/challenges-for-mitigating-bias-in-algorithmic-hiring/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Challenges%20for%20mitigating%20bias%20in%20algorithmic%20hiring&amp;rft.description=AI%20is%20increasingly%20common%20in%20the%20hiring%20process.&amp;rft.identifier=https%3A%2F%2Fwww.brookings.edu%2Fresearch%2Fchallenges-for-mitigating-bias-in-algorithmic-hiring%2F&amp;rft.aufirst=Manish&amp;rft.aulast=Raghavan&amp;rft.au=Manish%20Raghavan&amp;rft.au=Solon%20Barocas&amp;rft.date=2019-12-06&amp;rft.language=en-US"></span>
</div></td>
    <td></td>
  </tr>  
  <tr>
    <td>2/15</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Booth, Robert. “Uber Drivers to Launch Legal Bid to Uncover App’s Algorithm.” <i>The Guardian</i>, 20 July 2020, <a href="http://www.theguardian.com/technology/2020/jul/20/uber-drivers-to-launch-legal-bid-to-uncover-apps-algorithm">http://www.theguardian.com/technology/2020/jul/20/uber-drivers-to-launch-legal-bid-to-uncover-apps-algorithm</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Uber%20drivers%20to%20launch%20legal%20bid%20to%20uncover%20app's%20algorithm&amp;rft.description=Union%20wants%20ride-sharing%20firm%20to%20increase%20transparency%20and%20disclose%20how%20data%20is%20used&amp;rft.identifier=http%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2020%2Fjul%2F20%2Fuber-drivers-to-launch-legal-bid-to-uncover-apps-algorithm&amp;rft.aufirst=Robert&amp;rft.aulast=Booth&amp;rft.au=Robert%20Booth&amp;rft.date=2020-07-20&amp;rft.language=en"></span>
  <div class="csl-entry">Crawford, Kate. “Labor.” <i>The Atlas of AI</i>, Yale University Press, 2021, pp. 53–87. <i>JSTOR</i>, <a href="https://doi.org/10.2307/j.ctv1ghv45t.5">https://doi.org/10.2307/j.ctv1ghv45t.5</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-0-300-20957-0&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Labor&amp;rft.publisher=Yale%20University%20Press&amp;rft.series=Power%2C%20Politics%2C%20and%20the%20Planetary%20Costs%20of%20Artificial%20Intelligence&amp;rft.aufirst=Kate&amp;rft.aulast=Crawford&amp;rft.au=Kate%20Crawford&amp;rft.date=2021&amp;rft.pages=53-87&amp;rft.spage=53&amp;rft.epage=87&amp;rft.isbn=978-0-300-20957-0"></span>
  <div class="csl-entry">Gurley, Lauren Kaori. “Amazon Drivers Are Instructed to Drive Recklessly to Meet Delivery Quotas.” <i>VICE</i>, 6 May 2021, <a href="https://www.vice.com/en/article/xgxx54/amazon-drivers-are-instructed-to-drive-recklessly-to-meet-delivery-quotas">https://www.vice.com/en/article/xgxx54/amazon-drivers-are-instructed-to-drive-recklessly-to-meet-delivery-quotas</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Amazon%20Drivers%20Are%20Instructed%20to%20Drive%20Recklessly%20to%20Meet%20Delivery%20Quotas&amp;rft.description=Drivers%20say%20they%E2%80%99re%20being%20asked%20to%20quietly%20disable%20a%20driver%20safety%20system%20partway%20through%20their%20shifts.&amp;rft.identifier=https%3A%2F%2Fwww.vice.com%2Fen%2Farticle%2Fxgxx54%2Famazon-drivers-are-instructed-to-drive-recklessly-to-meet-delivery-quotas&amp;rft.aufirst=Lauren%20Kaori&amp;rft.aulast=Gurley&amp;rft.au=Lauren%20Kaori%20Gurley&amp;rft.date=2021-05-06&amp;rft.language=en"></span>
  <div class="csl-entry">The Financial Times. <i>The Uber Game</i>. 2020, <a href="https://ig.ft.com/uber-game">https://ig.ft.com/uber-game</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=The%20Uber%20Game&amp;rft.description=A%20news%20game%20based%20on%20interviews%20with%20dozens%20of%20Uber%20drivers&amp;rft.identifier=https%3A%2F%2Fig.ft.com%2Fuber-game&amp;rft.aucorp=The%20Financial%20Times&amp;rft.au=undefined&amp;rft.date=2020&amp;rft.language=en-GB"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>2/17</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Eubanks, Virginia. “A Child Abuse Prediction Model Fails Poor Families.” <i>Wired</i>, Jan. 2018. <i>www.wired.com</i>, <a href="https://www.wired.com/story/excerpt-from-automating-inequality/">https://www.wired.com/story/excerpt-from-automating-inequality/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=A%20Child%20Abuse%20Prediction%20Model%20Fails%20Poor%20Families&amp;rft.source=Wired&amp;rft.description=Why%20Pittsburgh%E2%80%99s%20predictive%20analytics%20misdiagnoses%20child%20maltreatment%20and%20prescribes%20the%20wrong%20solutions&amp;rft.identifier=https%3A%2F%2Fwww.wired.com%2Fstory%2Fexcerpt-from-automating-inequality%2F&amp;rft.aufirst=Virginia&amp;rft.aulast=Eubanks&amp;rft.au=Virginia%20Eubanks&amp;rft.date=2018-01-15&amp;rft.issn=1059-1028&amp;rft.language=en-US"></span>
  <div class="csl-entry">Gilman, Michele. <i>Poverty Lawgorithms</i>. Data &amp; Society, 15 Sept. 2020, <a href="https://datasociety.net/library/poverty-lawgorithms/">https://datasociety.net/library/poverty-lawgorithms/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Poverty%20Lawgorithms&amp;rft.aufirst=Michele&amp;rft.aulast=Gilman&amp;rft.au=Michele%20Gilman&amp;rft.date=2020-09-15&amp;rft.language=en-US"></span>
  <div class="csl-entry">Henriques-Gomes, Luke. “The Automated System Leaving Welfare Recipients Cut off with Nowhere to Turn.” <i>The Guardian</i>, 16 Oct. 2019, <a href="http://www.theguardian.com/technology/2019/oct/16/automated-messages-welfare-australia-system">http://www.theguardian.com/technology/2019/oct/16/automated-messages-welfare-australia-system</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=newspaperArticle&amp;rft.title=The%20automated%20system%20leaving%20welfare%20recipients%20cut%20off%20with%20nowhere%20to%20turn&amp;rft.source=the%20Guardian&amp;rft.description=In%2012%20months%2C%20Australian%20welfare%20payments%20were%20stopped%20an%20extra%201m%20times%20thanks%20to%20automated%20technologies.%20Money%20is%20stopped%20first%20and%20questions%20asked%20later%2C%20causing%20untold%20misery&amp;rft.identifier=http%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2019%2Foct%2F16%2Fautomated-messages-welfare-australia-system&amp;rft.aufirst=Luke&amp;rft.aulast=Henriques-Gomes&amp;rft.au=Luke%20Henriques-Gomes&amp;rft.date=2019-10-16&amp;rft.language=en"></span>
  <div class="csl-entry">Sudhir, K., and Shyam Sunder. “What Happens When a Billion Identities Are Digitized?” <i>Yale Insights</i>, 27 Mar. 2020, <a href="https://insights.som.yale.edu/insights/what-happens-when-billion-identities-are-digitized">https://insights.som.yale.edu/insights/what-happens-when-billion-identities-are-digitized</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=What%20Happens%20When%20a%20Billion%20Identities%20Are%20Digitized%3F&amp;rft.description=The%20success%20of%20India%E2%80%99s%20Aadhaar%2C%20a%20biometrically%20secured%20national%20identification%20system%2C%20has%20ignited%20a%20debate%20over%20whether%20any%20entity%2C%20public%20or%20private%2C%20should%20have%20the%20ability%20to%20pool%20our%20full%20digital%20profiles.&amp;rft.identifier=https%3A%2F%2Finsights.som.yale.edu%2Finsights%2Fwhat-happens-when-billion-identities-are-digitized&amp;rft.aufirst=K.&amp;rft.aulast=Sudhir&amp;rft.au=K.%20Sudhir&amp;rft.au=Shyam%20Sunder&amp;rft.date=2020-03-27&amp;rft.language=en"></span>
</div></td>
    <td></td>
  </tr>  
  <tr>
    <td>2/22</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Chen, Irene Y., et al. “Ethical Machine Learning in Healthcare.” <i>Annual Review of Biomedical Data Science</i>, vol. 4, no. 1, 2021, p. null. <i>Annual Reviews</i>, <a href="https://doi.org/10.1146/annurev-biodatasci-092820-114757">https://doi.org/10.1146/annurev-biodatasci-092820-114757</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1146%2Fannurev-biodatasci-092820-114757&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Ethical%20Machine%20Learning%20in%20Healthcare&amp;rft.jtitle=Annual%20Review%20of%20Biomedical%20Data%20Science&amp;rft.volume=4&amp;rft.issue=1&amp;rft.aufirst=Irene%20Y.&amp;rft.aulast=Chen&amp;rft.au=Irene%20Y.%20Chen&amp;rft.au=Emma%20Pierson&amp;rft.au=Sherri%20Rose&amp;rft.au=Shalmali%20Joshi&amp;rft.au=Kadija%20Ferryman&amp;rft.au=Marzyeh%20Ghassemi&amp;rft.date=2021&amp;rft.pages=null"></span>
  <div class="csl-entry">Obermeyer, Ziad, et al. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” <i>Science</i>, vol. 366, no. 6464, American Association for the Advancement of Science, Oct. 2019, pp. 447–53. <i>www-science-org.offcampus.lib.washington.edu (Atypon)</i>, <a href="https://doi.org/10.1126/science.aax2342">https://doi.org/10.1126/science.aax2342</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.aax2342&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Dissecting%20racial%20bias%20in%20an%20algorithm%20used%20to%20manage%20the%20health%20of%20populations&amp;rft.jtitle=Science&amp;rft.volume=366&amp;rft.issue=6464&amp;rft.aufirst=Ziad&amp;rft.aulast=Obermeyer&amp;rft.au=Ziad%20Obermeyer&amp;rft.au=Brian%20Powers&amp;rft.au=Christine%20Vogeli&amp;rft.au=Sendhil%20Mullainathan&amp;rft.date=2019-10-25&amp;rft.pages=447-453&amp;rft.spage=447&amp;rft.epage=453"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>2/24</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Jeffrey Mervis. “Can a Set of Equations Keep U.S. Census Data Private?” <i>ScienceInsider</i>, 4 Jan. 2019, <a href="https://www.science.org/content/article/can-set-equations-keep-us-census-data-private">https://www.science.org/content/article/can-set-equations-keep-us-census-data-private</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=blogPost&amp;rft.title=Can%20a%20set%20of%20equations%20keep%20U.S.%20census%20data%20private%3F&amp;rft.description=Census%20Bureau%20embraces%20differential%20privacy%20in%20latest%20attempt%20to%20ensure%20confidentiality%20without%20sacrificing%20data%20quality&amp;rft.identifier=https%3A%2F%2Fwww.science.org%2Fcontent%2Farticle%2Fcan-set-equations-keep-us-census-data-private&amp;rft.aucorp=Jeffrey%20Mervis&amp;rft.au=undefined&amp;rft.date=2019-01-04&amp;rft.language=en"></span>
  <div class="csl-entry">Wezerek, Gus, and David Van Riper. “Opinion | Changes to the Census Could Make Small Towns Disappear.” <i>The New York Times</i>, 6 Feb. 2020. <i>NYTimes.com</i>, <a href="https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html">https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=newspaperArticle&amp;rft.title=Opinion%20%7C%20Changes%20to%20the%20Census%20Could%20Make%20Small%20Towns%20Disappear&amp;rft.source=The%20New%20York%20Times&amp;rft.description=An%20algorithm%20intended%20to%20keep%20responses%20anonymous%20could%20erase%20minorities%20and%20rural%20communities%20%E2%80%94%20and%20deprive%20them%20of%20federal%20funding.&amp;rft.identifier=https%3A%2F%2Fwww.nytimes.com%2Finteractive%2F2020%2F02%2F06%2Fopinion%2Fcensus-algorithm-privacy.html&amp;rft.aufirst=Gus&amp;rft.aulast=Wezerek&amp;rft.au=Gus%20Wezerek&amp;rft.au=David%20Van%20Riper&amp;rft.date=2020-02-06&amp;rft.issn=0362-4331&amp;rft.language=en-US"></span>
  <div class="csl-entry">Wolford, Ben. “What Is GDPR, the EU’s New Data Protection Law?” <i>GDPR.Eu</i>, 7 Nov. 2018, <a href="https://gdpr.eu/what-is-gdpr/">https://gdpr.eu/what-is-gdpr/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=webpage&amp;rft.title=What%20is%20GDPR%2C%20the%20EU%E2%80%99s%20new%20data%20protection%20law%3F&amp;rft.description=What%20is%20the%20GDPR%3F%20Europe%E2%80%99s%20new%20data%20privacy%20and%20security%20law%20includes%20hundreds%20of%20pages%E2%80%99%20worth%20of%20new%20requirements%20for%20organizations%20around%20the%20world.%20This%20GDPR%20overview%20will%20help...&amp;rft.identifier=https%3A%2F%2Fgdpr.eu%2Fwhat-is-gdpr%2F&amp;rft.aufirst=Ben&amp;rft.aulast=Wolford&amp;rft.au=Ben%20Wolford&amp;rft.date=2018-11-07&amp;rft.language=en-US"></span>
  <div class="csl-entry">Wood, Alexandra, et al. “Differential Privacy: A Primer for a Non-Technical Audience.” <i>Vanderbilt Journal of Entertainment &amp;amp; Technology Law</i>, vol. 21, no. 1, 2018, pp. 209–75.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Differential%20privacy%3A%20A%20primer%20for%20a%20non-technical%20audience&amp;rft.jtitle=Vanderbilt%20Journal%20of%20Entertainment%20%26amp%3B%20Technology%20Law&amp;rft.volume=21&amp;rft.issue=1&amp;rft.aufirst=Alexandra&amp;rft.aulast=Wood&amp;rft.au=Alexandra%20Wood&amp;rft.au=Micah%20Altman&amp;rft.au=Aaron%20Bembenek&amp;rft.au=Mark%20Bun&amp;rft.au=Marco%20Gaboardi&amp;rft.au=James%20Honaker&amp;rft.au=Kobbi%20Nissim&amp;rft.au=David%20R.%20OBrien&amp;rft.au=Thomas%20Steinke&amp;rft.au=Salil%20Vadhan&amp;rft.date=2018&amp;rft.pages=209-275&amp;rft.spage=209&amp;rft.epage=275"></span>
</div></td>
    <td></td>
  </tr>  
  <tr>
    <td>3/1</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Green, Ben. “The False Promise of Risk Assessments: Epistemic Reform and the Limits of Fairness.” <i>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</i>, Association for Computing Machinery, 2020, pp. 594–606. <i>ACM Digital Library</i>, <a href="https://doi.org/10.1145/3351095.3372869">https://doi.org/10.1145/3351095.3372869</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F3351095.3372869&amp;rft_id=urn%3Aisbn%3A978-1-4503-6936-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=The%20false%20promise%20of%20risk%20assessments%3A%20epistemic%20reform%20and%20the%20limits%20of%20fairness&amp;rft.btitle=Proceedings%20of%20the%202020%20Conference%20on%20Fairness%2C%20Accountability%2C%20and%20Transparency&amp;rft.place=New%20York%2C%20NY%2C%20USA&amp;rft.publisher=Association%20for%20Computing%20Machinery&amp;rft.series=FAT*%20'20&amp;rft.aufirst=Ben&amp;rft.aulast=Green&amp;rft.au=Ben%20Green&amp;rft.date=2020-01-27&amp;rft.pages=594%E2%80%93606&amp;rft.spage=594&amp;rft.epage=606&amp;rft.isbn=978-1-4503-6936-7"></span>
  <div class="csl-entry">Hooker, Sara. “Moving beyond ‘Algorithmic Bias Is a Data Problem.’” <i>Patterns</i>, vol. 2, no. 4, Elsevier, Apr. 2021. <i>www.cell.com</i>, <a href="https://doi.org/10.1016/j.patter.2021.100241">https://doi.org/10.1016/j.patter.2021.100241</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patter.2021.100241&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Moving%20beyond%20%E2%80%9Calgorithmic%20bias%20is%20a%20data%20problem%E2%80%9D&amp;rft.jtitle=Patterns&amp;rft.stitle=PATTER&amp;rft.volume=2&amp;rft.issue=4&amp;rft.aufirst=Sara&amp;rft.aulast=Hooker&amp;rft.au=Sara%20Hooker&amp;rft.date=2021-04-09&amp;rft.issn=2666-3899&amp;rft.language=English"></span>
  <div class="csl-entry">Raji, Inioluwa Deborah, and Joy Buolamwini. “Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products.” <i>Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</i>, Association for Computing Machinery, 2019, pp. 429–35. <i>ACM Digital Library</i>, <a href="https://doi.org/10.1145/3306618.3314244">https://doi.org/10.1145/3306618.3314244</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F3306618.3314244&amp;rft_id=urn%3Aisbn%3A978-1-4503-6324-2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Actionable%20Auditing%3A%20Investigating%20the%20Impact%20of%20Publicly%20Naming%20Biased%20Performance%20Results%20of%20Commercial%20AI%20Products&amp;rft.btitle=Proceedings%20of%20the%202019%20AAAI%2FACM%20Conference%20on%20AI%2C%20Ethics%2C%20and%20Society&amp;rft.place=New%20York%2C%20NY%2C%20USA&amp;rft.publisher=Association%20for%20Computing%20Machinery&amp;rft.series=AIES%20'19&amp;rft.aufirst=Inioluwa%20Deborah&amp;rft.aulast=Raji&amp;rft.au=Inioluwa%20Deborah%20Raji&amp;rft.au=Joy%20Buolamwini&amp;rft.date=2019-01-27&amp;rft.pages=429%E2%80%93435&amp;rft.spage=429&amp;rft.epage=435&amp;rft.isbn=978-1-4503-6324-2"></span>
  <div class="csl-entry">Upchurch, Tom. “To Work for Society, Data Scientists Need a Hippocratic Oath with Teeth.” <i>Wired UK</i>. <i>www.wired.co.uk</i>, <a href="https://www.wired.co.uk/article/data-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction">https://www.wired.co.uk/article/data-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction</a>. Accessed 10 May 2021.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=To%20work%20for%20society%2C%20data%20scientists%20need%20a%20hippocratic%20oath%20with%20teeth&amp;rft.source=Wired%20UK&amp;rft.description=Weapons%20of%20Math%20Destruction%20author%20Cathy%20O%E2%80%99Neil%20explains%20how%20a%20data%20science%20ethical%20code%20should%20be%20developed%20and%20implemented.%20So%20far%2C%20she%20says%2C%20none%20that%20have%20been%20proposed%20are%20good%20enough&amp;rft.identifier=https%3A%2F%2Fwww.wired.co.uk%2Farticle%2Fdata-ai-ethics-hippocratic-oath-cathy-o-neil-weapons-of-math-destruction&amp;rft.aufirst=Tom&amp;rft.aulast=Upchurch&amp;rft.au=Tom%20Upchurch&amp;rft.issn=1357-0978&amp;rft.language=en-GB"></span>
</div></td>
    <td></td>
  </tr>
  <tr>
    <td>3/3</td>
    <td><div class="csl-bib-body" style="line-height: 1.7; margin-left: 2em; text-indent:-2em;">
  <div class="csl-entry">Campbell, Alexia Fernández. “How Tech Employees Are Pushing Silicon Valley to Put Ethics before Profit.” <i>Vox</i>, Oct. 2018, <a href="https://www.vox.com/technology/2018/10/18/17989482/google-amazon-employee-ethics-contracts">https://www.vox.com/technology/2018/10/18/17989482/google-amazon-employee-ethics-contracts</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=How%20tech%20employees%20are%20pushing%20Silicon%20Valley%20to%20put%20ethics%20before%20profit&amp;rft.source=Vox&amp;rft.description=Workers%20at%20Google%2C%20Amazon%2C%20and%20Microsoft%20are%20demanding%20input%20on%20the%20impact%20of%20the%20technology%20they%20build.&amp;rft.identifier=https%3A%2F%2Fwww.vox.com%2Ftechnology%2F2018%2F10%2F18%2F17989482%2Fgoogle-amazon-employee-ethics-contracts&amp;rft.aufirst=Alexia%20Fern%C3%A1ndez&amp;rft.aulast=Campbell&amp;rft.au=Alexia%20Fern%C3%A1ndez%20Campbell&amp;rft.date=2018-10-18&amp;rft.language=en"></span>
  <div class="csl-entry">Heilweil, Rebecca. “Facebook Is Taking a Hard Look at Racial Bias in Its Algorithms.” <i>Vox</i>, July 2020, <a href="https://www.vox.com/recode/2020/7/22/21334051/facebook-news-feed-instagram-algorithm-racial-bias-civil-rights-audit">https://www.vox.com/recode/2020/7/22/21334051/facebook-news-feed-instagram-algorithm-racial-bias-civil-rights-audit</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=Facebook%20is%20taking%20a%20hard%20look%20at%20racial%20bias%20in%20its%20algorithms&amp;rft.source=Vox&amp;rft.description=Following%20a%20rocky%20civil%20rights%20audit%2C%20Facebook%20is%20creating%20teams%20to%20make%20its%20platforms%20work%20better%20for%20everyone.&amp;rft.identifier=https%3A%2F%2Fwww.vox.com%2Frecode%2F2020%2F7%2F22%2F21334051%2Ffacebook-news-feed-instagram-algorithm-racial-bias-civil-rights-audit&amp;rft.aufirst=Rebecca&amp;rft.aulast=Heilweil&amp;rft.au=Rebecca%20Heilweil&amp;rft.date=2020-07-22&amp;rft.language=en"></span>
  <div class="csl-entry">Mullainathan, Sendhil. “Biased Algorithms Are Easier to Fix Than Biased People.” <i>The New York Times</i>, 6 Dec. 2019. <i>NYTimes.com</i>, <a href="https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html">https://www.nytimes.com/2019/12/06/business/algorithm-bias-fix.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=newspaperArticle&amp;rft.title=Biased%20Algorithms%20Are%20Easier%20to%20Fix%20Than%20Biased%20People&amp;rft.source=The%20New%20York%20Times&amp;rft.description=Racial%20discrimination%20by%20algorithms%20or%20by%20people%20is%20harmful%20%E2%80%94%20but%20that%E2%80%99s%20where%20the%20similarities%20end.&amp;rft.identifier=https%3A%2F%2Fwww.nytimes.com%2F2019%2F12%2F06%2Fbusiness%2Falgorithm-bias-fix.html&amp;rft.aufirst=Sendhil&amp;rft.aulast=Mullainathan&amp;rft.au=Sendhil%20Mullainathan&amp;rft.date=2019-12-06&amp;rft.issn=0362-4331&amp;rft.language=en-US"></span>
  <div class="csl-entry">NIckelsburg, Monica. “Washington State Lawmakers Seek to Ban Government from Using Discriminatory AI Tech.” <i>GeekWire</i>, Feb. 2021, <a href="https://www.geekwire.com/2021/washington-state-lawmakers-seek-ban-government-using-ai-tech-discriminates/">https://www.geekwire.com/2021/washington-state-lawmakers-seek-ban-government-using-ai-tech-discriminates/</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&amp;rft.type=magazineArticle&amp;rft.title=Washington%20state%20lawmakers%20seek%20to%20ban%20government%20from%20using%20discriminatory%20AI%20tech&amp;rft.source=GeekWire&amp;rft.description=Washington%20state%20could%20become%20a%20national%20leader%20in%20regulating%20the%20technologies%20of%20the%20future%2C%20thanks%20in%20part%20to%20a%20bill%20up%20for%20debate%20that%20would%20establish%20new%20guardrails%20on%20government%20use%20of%20artificial%E2%80%A6&amp;rft.identifier=https%3A%2F%2Fwww.geekwire.com%2F2021%2Fwashington-state-lawmakers-seek-ban-government-using-ai-tech-discriminates%2F&amp;rft.aufirst=Monica&amp;rft.aulast=NIckelsburg&amp;rft.au=Monica%20NIckelsburg&amp;rft.date=2021-02-13&amp;rft.language=en-US"></span>
  <div class="csl-entry">Richardson, Rashida. <i>Confronting Black Boxes: A Shadow Report of the New York City Automated Decision System Task Force</i>. AI Now Institute, 4 Dec. 2019, <a href="https://ainowinstitute.org/ads-shadowreport-2019.html">https:// ainowinstitute.org/ads-shadowreport-2019.html</a>.</div>
  <span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Confronting%20Black%20Boxes%3A%20A%20Shadow%20Report%20of%20the%20New%20York%20City%20Automated%20Decision%20System%20Task%20Force&amp;rft.aufirst=Rashida&amp;rft.aulast=Richardson&amp;rft.au=Rashida%20Richardson&amp;rft.date=2019-12-04"></span>
</div></td>
    <td></td>
  </tr>  
  <tr>
    <td>3/8</td>
    <td></td>
    <td>Final Presentations</td>
  </tr>
  <tr>
    <td>3/10</td>
    <td></td>
    <td>Final Presentations</td>
  </tr>  
            
</table>